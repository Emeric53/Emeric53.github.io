# 信息熵与交叉熵

我没有学过信息论这门学科，但是之前总听到过信息熵这个词，在深度学习领域的学习中，也总能听到交叉熵损失这个词，那么这些和“熵”有关的词到底是什么意思呢？在通过了一定的学习和理解以后，我来记录一下我对这两个概念的基本认识和理解。

## 信息量
在具备一定的概率论知识基础的前提下，我们知道在某个场景下，存在不同的接下来可能发生的事件，这些事件的发生是互斥的，A 事件发生，B 事件就不发生；这些事件各自发生的概率是非负的，且所有事件发生的概率之和为 1。

那么对于一个事件发生会产生的信息量，或者说惊讶量，应该是要和这个事件真正可能发生的概率成反比的。具体来说，如果一个事件发生的概率是 1,那么你知道他一定会发生了，他实际发生了也不会给你产生什么信息量，也就是说你一定也不惊讶这件事情的发生，他的信息量应该是0,而一个事件发生的概率特别低，例如 0.0001, 那么你现在的预设应该是这个事件基本不会发生，当他真正发生了，应该能给你带来很多的信息量，或者说你会特别惊讶这件事件真的发生了，极端一点，一个不可能发生的概率为 0 的事件发生了，你的惊讶程度应该是无穷大，信息量也是无穷大，只不过这个事件不可能发生，你也就不可能真的被这个无穷大惊讶到了。

那么从数学的角度总结一下，一个事件发生的概率区间应该是 [0,1]，信息量的区间应该是 [0,∞]，同时信息量和概率成反比，同时这个关系绝对不是线性的。有没有一个大家比较熟悉又符合这个关系的函数存在呢？负的对数函数 -log(x) 刚好就符合这个条件。

![-log(x)](-logx.png)

x 接近 0 时，y 接近正无穷；x 接近 1 时，y 接近 0。用这个函数就可以很好的对信息量进行定义，一个事件 $x_i$ 发生的信息量 $I(x_i)$ 可以定义为：

$$
I(x_i) = -log(x_i)
$$

## 信息熵
在有了事件发生的概率分布 $p(x_i)$ 之后，我们就可以结合各个事件的信息量 $I(x_i)$ 来计算信息熵 $H(p)$，信息熵可以看作是不同事件信息量的加权平均，这个权重也就是事件发生的概率，也就是你能获得的信息量或惊讶量的数学期望。

$$
H(p) = \sum_{i=1}^{n} p(x_i) I(x_i)
$$

信息熵的单位是比特，或者以其他单位为单位，比如以自然对数为底，那么信息熵的单位就是自然对数单位，对于一个恒定的概率分布 $p(x_i)$，信息熵 $H(p)$ 也是恒定的。

## 交叉熵与 KL 散度
那么交叉熵又是什么呢？我们不是上帝，我们无法预知未来，很多事情我们只能通过不同的方式来获取一个我们认为会发生事件的概率分布 $q(x_i)$，因此我们认为的事件的信息量也是和这个概率分布中事件发生的概率相关的。也就是说存在这么两个信息量

$$
I_{real}(x_i) = -log(q(x_i))
$$

$$
I_{pred}(x_i) = -log(p(x_i))
$$

我们对事件的信息量只能使用 $I_{pred}(x_i)$ 来描述，但是他们的发生背后又是实打实遵循着真实的概率分布 $p(x_i)$，那么交叉熵其实就是基于我们对事件信息量的描述 $I_{pred}(x_i)$ 和真实事件发生概率 $p(x_i)$ 相结合情况下获取的平均信息量。

$$
H(p, q) = \sum_{i=1}^{n} p(x_i) I_{pred}(x_i) = \sum_{i=1}^{n} p(x_i) (-log(q(x_i)))
$$

基于交叉熵和信息熵，我们还可以定义一个 KL 散度，这个散度表征了在使用我们自认为的概率分布 $q(x_i)$时，会产生的相较于本质的信息熵之外的额外惊讶度。三者的关系如下：

$$
H(p, q) = H(p) + D_{KL}(p || q)
$$

把公式展开，就能得到 KL 散度的公式：

$$
D_{KL}(p || q) = \sum_{i=1}^{n} p(x_i) (-log(q(x_i)) - (-log(p(x_i))))
$$

将重点放在公式的后面，也就是$-log(q(x_i)) - (-log(p(x_i)))$，这个项刚好就是该事件在假定概率 $q(x_i)$ 下的信息量减去在真实概率 $p(x_i)$ 下的信息量，而 KL 散度也就是我们使用假定概率描述时，获取的额外信息量或者惊讶度的期望。

所以，交叉熵和KL 散度，在优化问题上应该视为同一个东西，因为信息熵是恒定不变的，交叉熵或者 KL 散度都是由于你的假定概率分布和真实概率分布之间有差异，导致对不同时间定义的信息量和真实信息量存在差异，加权平均下来就对总的信息熵产生了影响。

KL 散度为什么不叫 KL 距离，是因为这个定义不满足距离的对称性，也就是$D_{KL}(p || q) \neq D_{KL}(q || p)$，从公式上就能很好理解，加权项不一致，而后面的信息量差值又互为相反数。

最后，说一下交叉熵损失函数的优化问题，本质也就是想让模型预测出来的分类概率分布和真实的概率分布之间尽可能的接近，从而降低信息量的差异，也就是降低 KL 散度，最终体现出来的就是交叉熵的降低，然而再怎么降低，交叉熵也不可能低于信息熵，因为信息熵是所有可能的事件发生的概率分布下，信息量的数学期望。

## 总结
本短文简单介绍了信息量，信息熵，交叉熵，KL 散度等概念，我对这部分概念的理解还不够深刻，但是我倾向与使用直觉的思维方式来深入理解，一方式是我不可能忘记为什么这么定义，二就是未来面对与这些内容相关的概念时，我能看到更本质也相对更简单的东西，因为很多复杂的概念都是包装出来的，希望我更多的看到本质。